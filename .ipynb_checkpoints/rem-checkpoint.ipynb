{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWXsObzt-NYn",
    "outputId": "a9da8503-8c5d-4fd5-c3ad-df3f530451dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python310\\lib\\site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\python310\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\python310\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pyLDAvis\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "EFos6L9qHO-0",
    "outputId": "b85080d7-a520-4ebb-d708-f08dc016bbb1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHGjC0GuGfNG"
   },
   "outputs": [],
   "source": [
    "# Import required libraries, functions and classes\n",
    "\n",
    "#Numpy and pandas for dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# nltk library for tokenization, lemmatizer, stopwords, pos tags and FreqDist\n",
    "# import string for punctuation and str manipulations\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,TweetTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "#Gensim library for LDA model creation . Corpora in gensim to create the id2word Dictionary and corpus of terms\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "#visualization using matplotlib and pyLDAvis for the LDA model viz\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "#import warnings to ignore deprecation warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category = DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSzwB1GP9OQt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel('part-2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQv7dixZiJnV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp1MvzGs9wvM",
    "outputId": "efb9cac8-0ad2-4dbb-f65d-54b5f43d0061"
   },
   "outputs": [],
   "source": [
    "df1 = df[['Head Line', 'Review', 'Places', 'Destination']]\n",
    "df1 = df1.dropna(how='all', inplace=False)\n",
    "null_rows = df1.isnull().sum(axis=1)\n",
    "print(null_rows)\n",
    "df1 = df1[null_rows == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGPOu_uN-Kwc"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  text = text.lower()\n",
    "  text = text.replace('\\n', ' ')\n",
    "  text = text.replace('_x000d_', ' ')\n",
    "  text = text.replace(',', ' ')\n",
    "  text = text.replace('.', ' ')\n",
    "  text = text.replace('?', ' ')\n",
    "  text = text.replace('!', ' ')\n",
    "  text = text.replace(';', ' ')\n",
    "  text = text.replace(':', ' ')\n",
    "  text = text.replace('(', ' ')\n",
    "  text = text.replace(')', ' ')\n",
    "  return text\n",
    "\n",
    "df['Review'] = df['Review'].astype(str)\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "df1['Review']= df['Review'][df['Destination'] == 'Mysore']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jqn3t-HPwbt"
   },
   "outputs": [],
   "source": [
    "reviews_df=df1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "W-uf8yMOFHNZ",
    "outputId": "1faf0116-c349-4791-ab35-d47e44ac7d4d"
   },
   "outputs": [],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9RFOeZyGMb_"
   },
   "source": [
    "### Task 2: Normalize casings for the review text and extract the text into a list for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0DfrYp4-FSY2",
    "outputId": "459e975c-53fe-446c-eb03-af20e24f3b2a"
   },
   "outputs": [],
   "source": [
    "#  Normalize the text - reduce to lower case\n",
    "review_list = [review.lower() for review in reviews_df[\"Review\"]]\n",
    "review_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq_H4EG-GThf"
   },
   "source": [
    "###Task 3:Tokenize the reviews using NLTKs word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxkDeBPrFeZ8",
    "outputId": "a7565ee4-3b2f-4324-f9cc-7cde304b4915"
   },
   "outputs": [],
   "source": [
    "# Tokenize the reviews\n",
    "\n",
    "rev_words = [word_tokenize(review) for review in review_list]\n",
    "print(rev_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8EFL3IDGYxU",
    "outputId": "20f2030f-9653-4e27-ca2b-056e588bee4b"
   },
   "outputs": [],
   "source": [
    "# POS tagging using NLTK pos tagger\n",
    "pos_tagged_review = [pos_tag(review) for review in rev_words]\n",
    "print(len(pos_tagged_review))\n",
    "print(pos_tagged_review[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWweFuIIHnPz",
    "outputId": "41776943-ed3e-4c10-a02b-aa60986b839f"
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(tagpattern='NN*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_lcncwqHy-a",
    "outputId": "cd263355-7fd5-4b12-ba2f-d52f2e297622"
   },
   "outputs": [],
   "source": [
    "# Limit the data to only terms with noun tags\n",
    "\n",
    "pos_noun_reviews = []\n",
    "for review in pos_tagged_review:\n",
    "    nouns=[]\n",
    "    for word_tuple in review:\n",
    "        if \"NN\" in word_tuple[1]:\n",
    "            nouns.append(word_tuple)\n",
    "    pos_noun_reviews.append(nouns)\n",
    "\n",
    "print(pos_noun_reviews[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZSyUDj0CH7zG",
    "outputId": "b2a0ecd6-36b1-4975-a429-5f82dc1e3ce9"
   },
   "outputs": [],
   "source": [
    "# Exclude any reviews that did not have any nouns as these reviews will be blank or empty sublists []\n",
    "\n",
    "pos_noun_reviews=[review for review in pos_noun_reviews if len(review)>=1]\n",
    "print(len(pos_noun_reviews), pos_noun_reviews[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T46HaSI0H-RO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gp3R8s_5IAnm"
   },
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kP8C9ID8IEH7",
    "outputId": "54b89aa7-2296-4553-e653-7386fb9df78b"
   },
   "outputs": [],
   "source": [
    "# Lemmatize the different forms of the nouns\n",
    "# POS tags not passed to lemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_words =[]\n",
    "for review in pos_noun_reviews:\n",
    "    lemma_word=[]\n",
    "    for word in review:\n",
    "        lemma_word.append(wnl.lemmatize(word[0]))\n",
    "    lemmatized_words.append(lemma_word)\n",
    "\n",
    "print(lemmatized_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOsZPRfbIEtB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmUw-RBwIMuw"
   },
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZOj0VjefIP8D"
   },
   "outputs": [],
   "source": [
    "# The o/p from lemmatizer still has many composite words that still contain emojis , special characters etc.\n",
    "# Using tweet tokenizer for isolating them better.\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "\n",
    "#Create list of stopwords with punctuations.Manually added token ['\\s'] as this is usually seperated in tokenize\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = stop_words+list(string.punctuation)+[\"\\'s\"]\n",
    "\n",
    "filtered_rev_words=[]\n",
    "\n",
    "for review in lemmatized_words:\n",
    "    filter_words=[]\n",
    "    for words in review:\n",
    "        rev_words = []\n",
    "        rev_words = tweet_tokenize.tokenize(words)\n",
    "        for word in rev_words:\n",
    "            if word not in stop_words:\n",
    "                filter_words.append(word)\n",
    "    filtered_rev_words.append(filter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jqDpdKPIQhz",
    "outputId": "044d6589-d6a2-4889-ae4a-492d51a104e7"
   },
   "outputs": [],
   "source": [
    "# Exclude any reviews that contained only stopwords as these reviews will be blank or empty sublists []\n",
    "filtered_rev_words=[review for review in filtered_rev_words if len(review)>=1]\n",
    "print(len(filtered_rev_words),filtered_rev_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w5HfqMGoIYem",
    "outputId": "5c080c76-f53c-4a4e-e628-9fdbc09fdf52"
   },
   "outputs": [],
   "source": [
    "# Barplot to visualize the 100 most common words using FreqDist and barplots\n",
    "\n",
    "list_of_words = [word for review in filtered_rev_words for word in review]\n",
    "common_word_freq=FreqDist(list_of_words).most_common(100)\n",
    "word_list = common_word_freq[::-1]\n",
    "\n",
    "words,freq = [],[]\n",
    "for word in word_list:\n",
    "    words.append(word[0])\n",
    "    freq.append(word[1])\n",
    "x=np.array(words)\n",
    "y=np.array(freq)\n",
    "\n",
    "plt.figure(figsize=(20,22))\n",
    "plt.barh(x,y,color=\"lightblue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvhKzm0rIbXU",
    "outputId": "59814ba0-37c8-4d8a-ce9f-a99f4e767017"
   },
   "outputs": [],
   "source": [
    "print(common_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3-eLvG_Iek0"
   },
   "outputs": [],
   "source": [
    "# Revising the stopwords based of above analysis\n",
    "stop_words_inclusions = [\"...\",\"..\",'phone','good','bad','lenovo','k8','note','product',\n",
    "                         'mobile','hai','please','pls','star','hi','ho','ok','superb','handset']\n",
    "stop_words = stop_words + stop_words_inclusions\n",
    "\n",
    "\n",
    "#isalnum() to remove emoji an isnumeric() to remove only number tokens present in the list\n",
    "#len(word)!=1 will eliminate all one letter tokens such as 'u','i' etc.\n",
    "final_rev_words = []\n",
    "for review in filtered_rev_words:\n",
    "    stopwords_removed_review=[]\n",
    "    for word in review:\n",
    "        if word not in stop_words and word.isalnum() and (not word.isnumeric()) and len(word)!=1:\n",
    "            stopwords_removed_review.append(word)\n",
    "    final_rev_words.append(stopwords_removed_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m0Xkk8SRIhON",
    "outputId": "3448f399-330b-4510-a1f3-e677402618e2"
   },
   "outputs": [],
   "source": [
    "# Clearing any reviews which are now empty lists after removal of revised stop words\n",
    "final_rev_words=[review for review in final_rev_words if len(review)>=1]\n",
    "print(len(final_rev_words),final_rev_words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WhMK8NaOIjB7",
    "outputId": "228605a5-9ad0-4e1f-d413-92817753886c"
   },
   "outputs": [],
   "source": [
    "# Barplot to visualize the 100 most common words using FreqDist and barplots\n",
    "\n",
    "list_of_words = [word for review in final_rev_words for word in review]\n",
    "word_freq=FreqDist(list_of_words).most_common(100)\n",
    "word_list_2 = word_freq[::-1]\n",
    "\n",
    "words,freq = [],[]\n",
    "for word in word_list_2:\n",
    "    words.append(word[0])\n",
    "    freq.append(word[1])\n",
    "x=np.array(words)\n",
    "y=np.array(freq)\n",
    "\n",
    "plt.figure(figsize=(20,22))\n",
    "plt.barh(x,y,color=\"plum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qp07EBpcIlOX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA85O9GrI0Aw"
   },
   "source": [
    "### Task 8: Create a topic model using LDA on the cleaned-up data with 12 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KzRfTULeI1jq",
    "outputId": "403b2d26-7a64-4553-f26d-edfc3d35c77f"
   },
   "outputs": [],
   "source": [
    "# First creating the id2word Dictionary and corpus of words required for the LDA topic model\n",
    "\n",
    "id2word = corpora.Dictionary(final_rev_words)\n",
    "\n",
    "corpus =[]\n",
    "for review in final_rev_words:\n",
    "    new = id2word.doc2bow(review)\n",
    "    corpus.append(new)\n",
    "\n",
    "print(corpus[:20],\"\\n\")\n",
    "print(\"No of reviews:\",len(corpus),\"\\n\")\n",
    "print(\"No of unique words:\",len(id2word),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6avTbNyI3TI"
   },
   "outputs": [],
   "source": [
    "# Create a topic model using LDA on the cleaned-up data with 12 topics\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=12,\n",
    "                                           random_state=47,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iAr2RM48I5O7",
    "outputId": "0e37694f-99c7-4658-bd17-896eb90146ce"
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bgzCeQ-LI-AH",
    "outputId": "c285b351-c248-41d5-fe3d-180a8c4c5968"
   },
   "outputs": [],
   "source": [
    "# Top terms for each topic.\n",
    "\n",
    "topics=[]\n",
    "topic_terms=[]\n",
    "for idx in range(12):\n",
    "    topics.append(\"Topic \"+ str(idx+1))\n",
    "    terms=[]\n",
    "    for term in lda_model.get_topic_terms(idx,topn=8):\n",
    "        terms.append(id2word[term[0]])\n",
    "    topic_terms.append(terms)\n",
    "\n",
    "for idx in range(10):\n",
    "    print(idx,topic_terms[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "BZRG7qv2JAwD",
    "outputId": "5cf0ba93-84ed-4f2d-e1c2-cd3c9b9b2c00"
   },
   "outputs": [],
   "source": [
    "df_topics = pd.DataFrame(topic_terms).transpose()\n",
    "df_topics.columns = topics\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6zCSOw6JeMa",
    "outputId": "dd3f1399-2e7a-4d8c-dbdf-9c68801a1f51"
   },
   "outputs": [],
   "source": [
    "# coherence of the model with the c_v metric?\n",
    "\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=final_rev_words, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFl-Y7DJJh9o"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
