{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWXsObzt-NYn",
    "outputId": "9ba8db9e-4d9d-43bf-8a86-7a58cece2274"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\python310\\lib\\site-packages (4.33.2)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\python310\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from transformers) (1.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\python310\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\python310\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests->transformers) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -treamlit (c:\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eSzwB1GP9OQt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_excel('part-2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQv7dixZiJnV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xp1MvzGs9wvM",
    "outputId": "9657205a-dccb-43b4-85be-48d263439125"
   },
   "outputs": [],
   "source": [
    "df1 = df[['Head Line', 'Review', 'Places', 'Destination']]\n",
    "df1 = df1.dropna(how='all', inplace=False)\n",
    "null_rows = df1.isnull().sum(axis=1)\n",
    "print(null_rows)\n",
    "df1 = df1[null_rows == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EGPOu_uN-Kwc",
    "outputId": "a2bdf03c-6a78-4255-90fd-d1daf4ccc650"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  text = text.lower()\n",
    "  text = text.replace('\\n', ' ')\n",
    "  text = text.replace(',', ' ')\n",
    "  text = text.replace('.', ' ')\n",
    "  text = text.replace('?', ' ')\n",
    "  text = text.replace('!', ' ')\n",
    "  text = text.replace(';', ' ')\n",
    "  text = text.replace(':', ' ')\n",
    "  text = text.replace('(', ' ')\n",
    "  text = text.replace(')', ' ')\n",
    "  return text\n",
    "\n",
    "df['Review'] = df['Review'].astype(str)\n",
    "df['Review'].replace(\"_x000d_ \", \"\\n\")\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "df1['Review']= df['Review'][df['Destination'] == 'Mysore']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jqn3t-HPwbt"
   },
   "outputs": [],
   "source": [
    "df1=df1[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEcRtaBd-FZx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Convert the 'Review' column to a list of strings\n",
    "reviews = df1['Review'].tolist()\n",
    "\n",
    "# Tokenize and embed the data using BERT\n",
    "embeddings = []\n",
    "\n",
    "for review in reviews:\n",
    "    # Split the review into lines\n",
    "    lines = review.split('\\n')\n",
    "    tokenized_lines = tokenizer(lines, truncation=True, padding=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**tokenized_lines)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        cls_embedding = hidden_states[:, 0, :].numpy()\n",
    "        embeddings.append(cls_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBNIfYhl-13r",
    "outputId": "f9c061e0-ef9b-42be-d9bc-9db456a543c3"
   },
   "outputs": [],
   "source": [
    "flattened_embeddings = [np.log(embedding.flatten() - np.min(embedding.flatten()) + 1) for embedding in embeddings]\n",
    "lda = LatentDirichletAllocation(n_components=10)  # Specify the number of topics\n",
    "topic_distributions = lda.fit_transform(flattened_embeddings)\n",
    "\n",
    "# Combine BERT embeddings and LDA topic distributions\n",
    "combined_features = np.concatenate((flattened_embeddings, topic_distributions), axis=1)\n",
    "\n",
    "# Apply autoencoders for dimensionality reduction\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(combined_features)\n",
    "\n",
    "autoencoder = MLPRegressor(hidden_layer_sizes=(512, 256, 128, 64, 32, 16, 8, 4, 2, 4, 8, 16, 32, 64, 128, 256, 512),\n",
    "                            activation='relu', random_state=42, max_iter=1000)\n",
    "autoencoder.fit(scaled_features, scaled_features)\n",
    "\n",
    "encoded_features = autoencoder.predict(scaled_features)\n",
    "\n",
    "# Apply K-means clustering on the encoded features\n",
    "num_clusters = 5  # Specify the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "cluster_labels = kmeans.fit_predict(encoded_features)\n",
    "\n",
    "# Print the cluster labels for each data point\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    print(f\"Data point {i+1} belongs to cluster {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iSnFzU-iyG0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
